{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268c66ce-21e4-4496-bc94-7984c289fea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Data Mining Project - Deliverable 2\n",
    "# Regression Modeling and Performance Evaluation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"=== Advanced Data Mining Project - Deliverable 2 ===\")\n",
    "print(\"Regression Modeling and Performance Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: DATA LOADING AND CLEANING (FROM DELIVERABLE 1)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n1. DATA LOADING AND CLEANING\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"Loading UCI Online Retail Dataset...\")\n",
    "\n",
    "# Try to load UCI Online Retail Dataset\n",
    "try:\n",
    "    # First try local file (fastest)\n",
    "    try:\n",
    "        df = pd.read_excel('Online_Retail.xlsx')\n",
    "        print(\"Loaded raw dataset from local Online_Retail.xlsx file\")\n",
    "        print(f\"Original dataset: {len(df)} records, {len(df.columns)} features\")\n",
    "    except FileNotFoundError:\n",
    "        # Fallback to UCI repository\n",
    "        print(\"Local file not found. Fetching from UCI repository...\")\n",
    "        from ucimlrepo import fetch_ucirepo\n",
    "        online_retail = fetch_ucirepo(id=352)\n",
    "        df = online_retail.data.features.copy()\n",
    "        print(f\"Dataset loaded from UCI repository\")\n",
    "        print(f\"Original dataset: {len(df)} records, {len(df.columns)} features\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"ucimlrepo not available. Please download Online_Retail.xlsx manually\")\n",
    "    print(\"From: https://archive.ics.uci.edu/dataset/352/online+retail\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Please download Online_Retail.xlsx from UCI repository\")\n",
    "    exit()\n",
    "\n",
    "# ============================================================================\n",
    "# DATA CLEANING PIPELINE (ADAPTED FROM DELIVERABLE 1)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n1.1 Data Cleaning Pipeline:\")\n",
    "original_shape = df.shape\n",
    "\n",
    "# Handle Missing Values\n",
    "print(f\"Missing values before cleaning:\")\n",
    "missing_before = df.isnull().sum()\n",
    "print(missing_before[missing_before > 0])\n",
    "\n",
    "# Remove transactions without CustomerID (needed for customer analysis)\n",
    "df_clean = df.dropna(subset=['CustomerID']).copy()\n",
    "print(f\"After removing missing CustomerID: {len(df_clean)} records\")\n",
    "\n",
    "# Remove missing descriptions if any\n",
    "if df_clean['Description'].isnull().sum() > 0:\n",
    "    df_clean = df_clean.dropna(subset=['Description'])\n",
    "    print(f\"After removing missing descriptions: {len(df_clean)} records\")\n",
    "\n",
    "# Handle Data Quality Issues\n",
    "print(f\"Cancellation transactions: {df_clean['InvoiceNo'].astype(str).str.startswith('C').sum()}\")\n",
    "print(f\"Negative quantities: {(df_clean['Quantity'] < 0).sum()}\")\n",
    "print(f\"Zero/negative prices: {(df_clean['UnitPrice'] <= 0).sum()}\")\n",
    "\n",
    "# Focus on positive transactions for regression modeling\n",
    "df_clean = df_clean[(df_clean['Quantity'] > 0) & (df_clean['UnitPrice'] > 0)].copy()\n",
    "print(f\"After removing negative quantities and zero prices: {len(df_clean)} records\")\n",
    "\n",
    "# Convert data types\n",
    "df_clean['CustomerID'] = df_clean['CustomerID'].astype(int)\n",
    "df_clean['InvoiceDate'] = pd.to_datetime(df_clean['InvoiceDate'])\n",
    "df_clean['Description'] = df_clean['Description'].str.strip().str.upper()\n",
    "\n",
    "# Create TotalAmount\n",
    "df_clean['TotalAmount'] = df_clean['Quantity'] * df_clean['UnitPrice']\n",
    "\n",
    "# Add temporal features\n",
    "df_clean['Year'] = df_clean['InvoiceDate'].dt.year\n",
    "df_clean['Month'] = df_clean['InvoiceDate'].dt.month\n",
    "df_clean['DayOfWeek'] = df_clean['InvoiceDate'].dt.dayofweek\n",
    "df_clean['Hour'] = df_clean['InvoiceDate'].dt.hour\n",
    "\n",
    "# Handle extreme outliers (cap at 99th percentile)\n",
    "for col in ['UnitPrice', 'TotalAmount']:\n",
    "    Q99 = df_clean[col].quantile(0.99)\n",
    "    extreme_outliers = df_clean[col] > Q99\n",
    "    if extreme_outliers.sum() > 0:\n",
    "        print(f\"Capping {extreme_outliers.sum()} extreme outliers in {col} at {Q99:.2f}\")\n",
    "        df_clean.loc[df_clean[col] > Q99, col] = Q99\n",
    "\n",
    "print(f\"\\nCleaning completed: {len(df_clean)} clean records\")\n",
    "print(f\"Data reduction: {((original_shape[0] - len(df_clean)) / original_shape[0] * 100):.1f}% records removed\")\n",
    "\n",
    "# Update main dataframe\n",
    "df = df_clean.copy()\n",
    "\n",
    "# Basic dataset overview\n",
    "print(f\"\\nCleaned Dataset Overview:\")\n",
    "print(f\"- Shape: {df.shape}\")\n",
    "print(f\"- Date range: {df['InvoiceDate'].min()} to {df['InvoiceDate'].max()}\")\n",
    "print(f\"- Unique customers: {df['CustomerID'].nunique()}\")\n",
    "print(f\"- Unique products: {df['StockCode'].nunique()}\")\n",
    "print(f\"- Countries: {df['Country'].nunique()}\")\n",
    "print(f\"- Total revenue: £{df['TotalAmount'].sum():,.2f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: ADVANCED FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n2. ADVANCED FEATURE ENGINEERING\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create a copy for feature engineering\n",
    "df_features = df.copy()\n",
    "\n",
    "# 2.1 Customer-Level Features\n",
    "print(\"\\n2.1 Creating Customer-Level Features...\")\n",
    "\n",
    "# Customer aggregation features\n",
    "customer_features = df_features.groupby('CustomerID').agg({\n",
    "    'InvoiceNo': 'nunique',           # Number of orders\n",
    "    'Quantity': ['sum', 'mean'],      # Total and average items\n",
    "    'TotalAmount': ['sum', 'mean'],   # Total and average spending\n",
    "    'InvoiceDate': ['min', 'max'],    # First and last purchase\n",
    "    'StockCode': 'nunique',           # Product diversity\n",
    "    'Country': 'first'                # Customer country\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "customer_features.columns = [\n",
    "    'OrderCount', 'TotalQuantity', 'AvgQuantity',\n",
    "    'TotalSpent', 'AvgOrderValue', 'FirstPurchase', 'LastPurchase',\n",
    "    'ProductDiversity', 'Country'\n",
    "]\n",
    "\n",
    "# Calculate customer lifetime and recency\n",
    "reference_date = df_features['InvoiceDate'].max()\n",
    "customer_features['CustomerLifespan'] = (\n",
    "    customer_features['LastPurchase'] - customer_features['FirstPurchase']\n",
    ").dt.days\n",
    "customer_features['Recency'] = (\n",
    "    reference_date - customer_features['LastPurchase']\n",
    ").dt.days\n",
    "\n",
    "# Purchase frequency (avoid division by zero)\n",
    "customer_features['PurchaseFrequency'] = customer_features['CustomerLifespan'] / customer_features['OrderCount']\n",
    "customer_features['PurchaseFrequency'] = customer_features['PurchaseFrequency'].fillna(0)\n",
    "\n",
    "print(f\"Customer features created: {customer_features.shape[1]} features for {customer_features.shape[0]} customers\")\n",
    "\n",
    "# 2.2 Product-Level Features\n",
    "print(\"\\n2.2 Creating Product-Level Features...\")\n",
    "\n",
    "product_features = df_features.groupby('StockCode').agg({\n",
    "    'UnitPrice': 'mean',\n",
    "    'Quantity': 'mean',\n",
    "    'CustomerID': 'nunique',\n",
    "    'TotalAmount': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "product_features.columns = ['AvgProductPrice', 'AvgProductQuantity', 'ProductPopularity', 'ProductRevenue']\n",
    "\n",
    "print(f\"Product features created: {product_features.shape[1]} features for {product_features.shape[0]} products\")\n",
    "\n",
    "# 2.3 Merge Features Back to Transaction Level\n",
    "print(\"\\n2.3 Merging Features to Transaction Level...\")\n",
    "\n",
    "# Merge customer features\n",
    "df_features = df_features.merge(customer_features.reset_index(), on='CustomerID', how='left')\n",
    "\n",
    "# Merge product features\n",
    "df_features = df_features.merge(product_features, left_on='StockCode', right_index=True, how='left')\n",
    "\n",
    "# 2.4 Create Additional Derived Features\n",
    "print(\"\\n2.4 Creating Derived Features...\")\n",
    "\n",
    "# Temporal features\n",
    "df_features['IsWeekend'] = (df_features['DayOfWeek'] >= 5).astype(int)\n",
    "df_features['IsBusinessHour'] = ((df_features['Hour'] >= 9) & (df_features['Hour'] <= 17)).astype(int)\n",
    "\n",
    "# Customer engagement metric\n",
    "df_features['CustomerEngagement'] = df_features['OrderCount'] * df_features['ProductDiversity']\n",
    "\n",
    "# Price-quantity interaction\n",
    "df_features['PriceQuantityInteraction'] = df_features['UnitPrice'] * df_features['Quantity']\n",
    "\n",
    "# Customer value per order\n",
    "df_features['CustomerValuePerOrder'] = df_features['TotalSpent'] / df_features['OrderCount']\n",
    "\n",
    "# Top countries (binary feature)\n",
    "top_countries = df_features['Country_x'].value_counts().head(5).index\n",
    "df_features['IsTopCountry'] = df_features['Country_x'].isin(top_countries).astype(int)\n",
    "\n",
    "# Price category\n",
    "df_features['PriceCategory'] = pd.cut(df_features['UnitPrice'], \n",
    "                                     bins=[0, 2, 5, 10, float('inf')], \n",
    "                                     labels=[0, 1, 2, 3])  # Numeric categories\n",
    "\n",
    "print(f\"Total features after engineering: {df_features.shape[1]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: REGRESSION DATASET PREPARATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n3. REGRESSION DATASET PREPARATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Select features for modeling (focus on numerical features that work well for regression)\n",
    "feature_columns = [\n",
    "    # Basic transaction features\n",
    "    'Quantity', 'UnitPrice', 'Hour', 'DayOfWeek', 'Month',\n",
    "    # Temporal features  \n",
    "    'IsWeekend', 'IsBusinessHour',\n",
    "    # Customer features\n",
    "    'OrderCount', 'AvgOrderValue', 'ProductDiversity', 'Recency', \n",
    "    'PurchaseFrequency', 'CustomerEngagement', 'CustomerValuePerOrder',\n",
    "    # Product features\n",
    "    'AvgProductPrice', 'AvgProductQuantity', 'ProductPopularity',\n",
    "    # Derived features\n",
    "    'IsTopCountry', 'PriceCategory'\n",
    "]\n",
    "\n",
    "print(f\"Selected features for modeling: {len(feature_columns)}\")\n",
    "for i, feature in enumerate(feature_columns, 1):\n",
    "    print(f\"  {i:2d}. {feature}\")\n",
    "\n",
    "# Prepare regression dataset\n",
    "X = df_features[feature_columns].copy()\n",
    "y = df_features['TotalAmount'].copy()\n",
    "\n",
    "# Handle any remaining missing values\n",
    "print(f\"\\nMissing values in features:\")\n",
    "missing_features = X.isnull().sum()\n",
    "if missing_features.sum() > 0:\n",
    "    print(missing_features[missing_features > 0])\n",
    "    X = X.fillna(X.median())\n",
    "    print(\"Missing values filled with median\")\n",
    "else:\n",
    "    print(\"No missing values found\")\n",
    "\n",
    "# Remove any infinite values\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "if X.isnull().sum().sum() > 0:\n",
    "    X = X.fillna(X.median())\n",
    "    print(\"Infinite values replaced\")\n",
    "\n",
    "print(f\"\\nFinal dataset for modeling:\")\n",
    "print(f\"- Features: {X.shape[1]}\")\n",
    "print(f\"- Samples: {X.shape[0]:,}\")\n",
    "print(f\"- Target range: £{y.min():.2f} to £{y.max():.2f}\")\n",
    "print(f\"- Target mean: £{y.mean():.2f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: DATA SPLITTING AND PREPROCESSING  \n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n4. DATA SPLITTING AND PREPROCESSING\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Feature scaling for regularized models\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Store feature names\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "print(\"Feature scaling completed for regularized models\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: REGRESSION MODEL DEVELOPMENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n5. REGRESSION MODEL DEVELOPMENT\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Initialize storage\n",
    "model_results = {}\n",
    "models = {}\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(name, model, X_train_data, X_test_data, y_train, y_test, use_cv=True):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    \n",
    "    # Fit model\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train_data, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train_data)\n",
    "    y_test_pred = model.predict(X_test_data)\n",
    "    \n",
    "    # Metrics\n",
    "    results = {\n",
    "        'train_r2': r2_score(y_train, y_train_pred),\n",
    "        'test_r2': r2_score(y_test, y_test_pred),\n",
    "        'train_rmse': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "        'train_mae': mean_absolute_error(y_train, y_train_pred),\n",
    "        'test_mae': mean_absolute_error(y_test, y_test_pred)\n",
    "    }\n",
    "    \n",
    "    # Cross-validation\n",
    "    if use_cv:\n",
    "        cv_scores = cross_val_score(model, X_train_data, y_train, cv=5, scoring='r2', n_jobs=-1)\n",
    "        results['cv_r2_mean'] = cv_scores.mean()\n",
    "        results['cv_r2_std'] = cv_scores.std()\n",
    "    else:\n",
    "        results['cv_r2_mean'] = results['test_r2']  # Fallback\n",
    "        results['cv_r2_std'] = 0.0\n",
    "    \n",
    "    # Store results\n",
    "    model_results[name] = results\n",
    "    models[name] = model\n",
    "    \n",
    "    print(f\"{name} Results:\")\n",
    "    print(f\"  Test R²: {results['test_r2']:.4f}\")\n",
    "    print(f\"  Test RMSE: £{results['test_rmse']:.2f}\")\n",
    "    print(f\"  Test MAE: £{results['test_mae']:.2f}\")\n",
    "    if use_cv:\n",
    "        print(f\"  CV R²: {results['cv_r2_mean']:.4f} ± {results['cv_r2_std']:.4f}\")\n",
    "    \n",
    "    return results, model\n",
    "\n",
    "# 5.1 Linear Regression (Baseline)\n",
    "linear_model = LinearRegression()\n",
    "linear_results, _ = evaluate_model(\"Linear Regression\", linear_model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# 5.2 Ridge Regression\n",
    "print(\"\\n5.2 Ridge Regression with Hyperparameter Tuning\")\n",
    "ridge_params = {'alpha': [0.1, 1.0, 10.0, 100.0]}\n",
    "ridge_grid = GridSearchCV(Ridge(random_state=42), ridge_params, cv=3, scoring='r2', n_jobs=-1)\n",
    "ridge_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Best Ridge alpha: {ridge_grid.best_params_['alpha']}\")\n",
    "best_ridge = Ridge(alpha=ridge_grid.best_params_['alpha'], random_state=42)\n",
    "ridge_results, _ = evaluate_model(\"Ridge Regression\", best_ridge, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "\n",
    "# 5.3 Lasso Regression  \n",
    "print(\"\\n5.3 Lasso Regression with Hyperparameter Tuning\")\n",
    "lasso_params = {'alpha': [0.1, 1.0, 10.0]}\n",
    "lasso_grid = GridSearchCV(Lasso(random_state=42, max_iter=2000), lasso_params, cv=3, scoring='r2', n_jobs=-1)\n",
    "lasso_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Best Lasso alpha: {lasso_grid.best_params_['alpha']}\")\n",
    "best_lasso = Lasso(alpha=lasso_grid.best_params_['alpha'], random_state=42, max_iter=2000)\n",
    "lasso_results, _ = evaluate_model(\"Lasso Regression\", best_lasso, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: MODEL COMPARISON AND EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n6. MODEL COMPARISON AND EVALUATION\") \n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame(model_results).T\n",
    "comparison_df = comparison_df[['test_r2', 'test_rmse', 'test_mae', 'cv_r2_mean', 'cv_r2_std']]\n",
    "comparison_df.columns = ['Test_R2', 'Test_RMSE', 'Test_MAE', 'CV_R2_Mean', 'CV_R2_Std']\n",
    "comparison_df = comparison_df.round(4)\n",
    "\n",
    "print(\"\\n6.1 Model Performance Summary:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Model':<20} {'Test R²':<10} {'RMSE':<10} {'MAE':<10} {'CV R²':<15}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model_name, row in comparison_df.iterrows():\n",
    "    print(f\"{model_name:<20} {row['Test_R2']:<10.4f} £{row['Test_RMSE']:<9.2f} £{row['Test_MAE']:<9.2f} {row['CV_R2_Mean']:.3f}±{row['CV_R2_Std']:.3f}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find best model\n",
    "best_model_name = comparison_df['Test_R2'].idxmax()\n",
    "best_r2 = comparison_df.loc[best_model_name, 'Test_R2']\n",
    "best_rmse = comparison_df.loc[best_model_name, 'Test_RMSE']\n",
    "\n",
    "print(f\"\\nBEST MODEL: {best_model_name}\")\n",
    "print(f\"   Test R²: {best_r2:.4f} (explains {best_r2*100:.1f}% of variance)\")\n",
    "print(f\"   Test RMSE: £{best_rmse:.2f} (average prediction error)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: VISUALIZATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n7. MODEL VISUALIZATION AND ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 7.1 Model Performance Comparison\n",
    "model_names = comparison_df.index\n",
    "r2_scores = comparison_df['Test_R2'].values\n",
    "rmse_scores = comparison_df['Test_RMSE'].values\n",
    "\n",
    "axes[0, 0].bar(model_names, r2_scores, color='skyblue', alpha=0.8)\n",
    "axes[0, 0].set_title('Model Comparison: Test R²', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('R² Score')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "for i, v in enumerate(r2_scores):\n",
    "    axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "axes[0, 1].bar(model_names, rmse_scores, color='lightcoral', alpha=0.8)\n",
    "axes[0, 1].set_title('Model Comparison: Test RMSE', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('RMSE (£)')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 7.2 Feature Importance (Ridge Coefficients)\n",
    "if 'Ridge Regression' in models:\n",
    "    ridge_model = models['Ridge Regression']\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': np.abs(ridge_model.coef_)\n",
    "    }).sort_values('Importance', ascending=True)\n",
    "    \n",
    "    top_features = feature_importance.tail(10)\n",
    "    axes[1, 0].barh(top_features['Feature'], top_features['Importance'], color='lightgreen', alpha=0.8)\n",
    "    axes[1, 0].set_title('Top 10 Feature Importance (Ridge)', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Absolute Coefficient Value')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 7.3 Actual vs Predicted for Best Model\n",
    "best_model_obj = models[best_model_name]\n",
    "if best_model_name == \"Linear Regression\":\n",
    "    y_pred = best_model_obj.predict(X_test)\n",
    "elif best_model_name == \"Random Forest\":\n",
    "    y_pred = best_model_obj.predict(X_test) \n",
    "else:\n",
    "    y_pred = best_model_obj.predict(X_test_scaled)\n",
    "\n",
    "axes[1, 1].scatter(y_test, y_pred, alpha=0.6, color='blue')\n",
    "axes[1, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[1, 1].set_xlabel('Actual TotalAmount (£)')\n",
    "axes[1, 1].set_ylabel('Predicted TotalAmount (£)')\n",
    "axes[1, 1].set_title(f'{best_model_name}: Actual vs Predicted\\nR² = {best_r2:.3f}', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: FEATURE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n8. FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if 'Ridge Regression' in models:\n",
    "    ridge_model = models['Ridge Regression']\n",
    "    feature_analysis = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Coefficient': ridge_model.coef_,\n",
    "        'Abs_Coefficient': np.abs(ridge_model.coef_)\n",
    "    }).sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    print(\"\\n8.1 Top 10 Most Important Features (Ridge Regression):\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, (_, row) in enumerate(feature_analysis.head(10).iterrows(), 1):\n",
    "        direction = \"increases\" if row['Coefficient'] > 0 else \"decreases\"\n",
    "        print(f\"{i:2d}. {row['Feature']:<25} {direction:<10} {row['Coefficient']:+8.4f}\")\n",
    "\n",
    "if 'Lasso Regression' in models:\n",
    "    lasso_model = models['Lasso Regression']\n",
    "    lasso_features = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Coefficient': lasso_model.coef_\n",
    "    })\n",
    "    selected_features = lasso_features[lasso_features['Coefficient'] != 0].sort_values('Coefficient', key=abs, ascending=False)\n",
    "    \n",
    "    print(f\"\\n8.2 Lasso Feature Selection:\")\n",
    "    print(f\"Selected {len(selected_features)} out of {len(feature_names)} features\")\n",
    "    if len(selected_features) > 0:\n",
    "        print(\"\\nSelected features:\")\n",
    "        for i, (_, row) in enumerate(selected_features.head(10).iterrows(), 1):\n",
    "            direction = \"increases\" if row['Coefficient'] > 0 else \"decreases\"\n",
    "            print(f\"{i:2d}. {row['Feature']:<25} {direction:<10} {row['Coefficient']:+8.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 9: BUSINESS INSIGHTS AND RECOMMENDATIONS  \n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n9. BUSINESS INSIGHTS AND RECOMMENDATIONS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(f\"\\n9.1 Model Performance Summary:\")\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Prediction Accuracy: {best_r2:.1%} of variance explained\")\n",
    "print(f\"Average Prediction Error: £{best_rmse:.2f}\")\n",
    "print(f\"Business Impact: Model can predict transaction values for revenue forecasting\")\n",
    "\n",
    "print(f\"\\n9.2 Key Predictive Features:\")\n",
    "if 'Ridge Regression' in models:\n",
    "    top_3_features = feature_analysis.head(3)\n",
    "    for i, (_, row) in enumerate(top_3_features.iterrows(), 1):\n",
    "        impact = \"increases\" if row['Coefficient'] > 0 else \"decreases\"\n",
    "        print(f\"{i}. {row['Feature']}: {impact} transaction value (coef: {row['Coefficient']:+.4f})\")\n",
    "\n",
    "print(f\"9.3 Business Recommendations:\")\n",
    "print(f\"• Revenue Forecasting: Use model for monthly/quarterly revenue predictions\")\n",
    "print(f\"• Customer Targeting: Focus on high-value customer segments identified\")  \n",
    "print(f\"• Pricing Strategy: Leverage price-quantity relationships discovered\")\n",
    "print(f\"• Inventory Planning: Prioritize products with high predictive importance\")\n",
    "print(f\"• Marketing Timing: Optimize campaigns based on temporal patterns found\")\n",
    "\n",
    "print(f\"\\n9.4 Model Deployment Readiness:\")\n",
    "print(f\"• Prediction Range: £{y_test.min():.2f} - £{y_test.max():.2f}\")\n",
    "print(f\"• Expected Accuracy: ±£{best_rmse:.2f} on average\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 10: MODEL PERSISTENCE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n10. SAVING TRAINED MODELS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Save the best model and preprocessing components\n",
    "try:\n",
    "    joblib.dump(models[best_model_name], 'best_regression_model.pkl')\n",
    "    joblib.dump(scaler, 'feature_scaler.pkl')\n",
    "    \n",
    "    # Save model metadata\n",
    "    model_metadata = {\n",
    "        'best_model': best_model_name,\n",
    "        'feature_names': feature_names,\n",
    "        'performance': {\n",
    "            'test_r2': float(best_r2),\n",
    "            'test_rmse': float(best_rmse),\n",
    "            'cv_r2_mean': float(comparison_df.loc[best_model_name, 'CV_R2_Mean']),\n",
    "            'cv_r2_std': float(comparison_df.loc[best_model_name, 'CV_R2_Std'])\n",
    "        },\n",
    "        'feature_importance': feature_analysis.head(10).to_dict('records') if 'Ridge Regression' in models else []\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open('model_metadata.json', 'w') as f:\n",
    "        json.dump(model_metadata, f, indent=2)\n",
    "    \n",
    "    print(\"Model artifacts saved successfully:\")\n",
    "    print(\"   • best_regression_model.pkl - Trained model\")  \n",
    "    print(\"   • feature_scaler.pkl - Feature preprocessing\")\n",
    "    print(\"   • model_metadata.json - Performance metrics\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error saving model files: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"PROJECT DELIVERABLE 2 COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"Dataset: {len(df):,} transactions processed and cleaned\")\n",
    "print(f\"Features: {len(feature_names)} engineered features created\") \n",
    "print(f\"Models: {len(models)} regression models trained and evaluated\")\n",
    "print(f\"Best Model: {best_model_name} (R² = {best_r2:.3f})\")\n",
    "print(f\"=\"*60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f487ccd-4c64-4bf1-adaa-d71fc761d222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
